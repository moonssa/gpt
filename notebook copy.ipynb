{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA potato is a starchy, tuberous crop from the perennial plant Solanum tuberosum. It is one of the most widely cultivated and consumed vegetables in the world. Potatoes are often boiled, baked, fried, or mashed as a side dish or ingredient in various dishes.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are a helpful assistant that defines words. Define this word: {word}\")\n",
    "llm=GPT4All(\n",
    " model=\"./falcon.bin\"\n",
    "\n",
    ")\n",
    "chain = prompt | llm\n",
    "chain.invoke({\n",
    "  \"word\":\"potato\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': [{'question': 'What year was Rome founded?',\n",
       "   'answers': [{'answer': '753 BC', 'correct': True},\n",
       "    {'answer': '476 AD', 'correct': False},\n",
       "    {'answer': '1000 BC', 'correct': False}]},\n",
       "  {'question': 'Who was the first emperor of Rome?',\n",
       "   'answers': [{'answer': 'Julius Caesar', 'correct': False},\n",
       "    {'answer': 'Augustus', 'correct': True},\n",
       "    {'answer': 'Nero', 'correct': False}]},\n",
       "  {'question': 'What famous structure in Rome was built by the ancient Romans and is still standing today?',\n",
       "   'answers': [{'answer': 'Colosseum', 'correct': True},\n",
       "    {'answer': 'Pantheon', 'correct': False},\n",
       "    {'answer': 'Trevi Fountain', 'correct': False}]}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# def get_weather(lon, lat):\n",
    "#   return \"hahaha\"\n",
    "\n",
    "function = {\n",
    "  \"name\": \"create_quiz\", \n",
    "  \"description\":\"function that takes list of questions and answers and returns a quiz\",\n",
    "  \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"required\":[\"questions\"],\n",
    "},\n",
    "}\n",
    "llm = ChatOpenAI(temperature=0.1).bind(\n",
    "  function_call = {\n",
    "    \"name\": \"create_quiz\"\n",
    "  },\n",
    "  functions=[\n",
    "    function\n",
    "  ]\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "  \"city\":\"rome\"\n",
    "})\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "import json\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in json.loads(response)['questions']:\n",
    "  print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'?\\nA: A starchy tuberous vegetable that is a staple food in many cultures, grown in rows in the ground and harvested when the plants die back. It is a popular food source and is used in a variety of dishes, including soups, salads, and fries.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, according to the text, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.'\n",
      " content='He wrote \"2+2=5\" in the dust on the table.' \n",
      " content='Julia is a character mentioned in the text. She is someone who Winston loves and cares for deeply, and he has a strong emotional connection to her.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Is Aaronson guilty?\\nAI: Yes, according to the text, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.\\nHuman: What message did he write in the table?\\nAI: He wrote \"2+2=5\" in the dust on the table.\\nHuman: Who is Julia?\\nAI: Julia is a character mentioned in the text. She is someone who Winston loves and cares for deeply, and he has a strong emotional connection to her.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result=chain.invoke(question)\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})\n",
    "    return result\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(return_return_messages= True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chap_1.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    ")\n",
    "ans1 = invoke_chain(\"Is Aaronson guilty?\")\n",
    "ans2 = invoke_chain(\"What message did he write in the table?\")\n",
    "ans3 = invoke_chain(\"Who is Julia?\")\n",
    "print(f\"{ans1}\\n {ans2} \\n {ans3}\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result=chain.invoke(question)\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})\n",
    "    return result\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(return_return_messages= True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chap_1.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    ")\n",
    "ans1 = invoke_chain(\"Is Aaronson guilty?\")\n",
    "ans2 = invoke_chain(\"What message did he write in the table?\")\n",
    "ans3 = invoke_chain(\"Who is Julia?\")\n",
    "print(f\"{ans1}\\n {ans2} \\n {ans3}\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result=chain.invoke(question)\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})\n",
    "    return result\n",
    "\n",
    "def save_message( question):\n",
    "    print(question)\n",
    "    # print(content)\n",
    "    #  memory.save_context({\"input\":question}, {\"output\":content})\n",
    "\n",
    "memory = ConversationBufferMemory(return_return_messages= True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chap_1.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    \n",
    ")\n",
    "result = chain.invoke(\"Is Aaronson guilty?\")\n",
    "\n",
    "result\n",
    "\n",
    "# ans1 = invoke_chain(\"Is Aaronson guilty?\")\n",
    "# ans2 = invoke_chain(\"What message did he write in the table?\")\n",
    "# ans3 = invoke_chain(\"Who is Julia?\")\n",
    "# print(f\"{ans1}\\n {ans2} \\n {ans3}\")\n",
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant. ......)]'), HumanMessage(content='Is Aaronson guilty?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "def plus(a,b):\n",
    "  return a+b\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "agent = initialize_agent(\n",
    "  llm=llm, \n",
    "  verbose=True,\n",
    "  agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "  tools=[\n",
    "  StructuredTool.from_function(\n",
    "    func=plus, \n",
    "    name=\"Sum calculator\", \n",
    "    description=\"Use this to perform sums of two numbers. This tool take two arguments, both should be numbers\",)\n",
    "])\n",
    "\n",
    "prompt=\"Cost of $355.39 + $924.87 + $721,2 + $1940.29 + $573.63 + $65.72 + $35 + $552.00 + $76.16 + $29.12\"\n",
    "agent.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to add up all the given numbers to find the total cost.\n",
      "Action: Sum calculator\n",
      "Action Input: 355.39,924.87\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1280.26\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 1280.26,721.2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m2001.46\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 2001.46,1940.29\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m3941.75\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 3941.75,573.63\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4515.38\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 4515.38,65.72\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4581.1\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 4581.1,35\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4616.1\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 4616.1,552.00\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5168.1\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to continue adding the remaining numbers to the total.\n",
      "Action: Sum calculator\n",
      "Action Input: 5168.1,76.16\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5244.26\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to add the last number to the total to get the final answer.\n",
      "Action: Sum calculator\n",
      "Action Input: 5244.26,29.12\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m5273.38\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: $5273.38\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Cost of $355.39 + $924.87 + $721,2 + $1940.29 + $573.63 + $65.72 + $35 + $552.00 + $76.16 + $29.12',\n",
       " 'output': '$5273.38'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "def plus(inputs):\n",
    "  a,b = inputs.split(\",\")\n",
    "  return float(a)+float(b)\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "agent = initialize_agent(\n",
    "  llm=llm, \n",
    "  verbose=True,\n",
    "  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "  handle_parsing_errors=True,\n",
    "  tools=[\n",
    "  Tool.from_function(\n",
    "    func=plus, \n",
    "    name=\"Sum calculator\", \n",
    "    description=\"Use this to perform sums of two numbers. Use this  tool by sending a pair of number separated by comma. \\n Example:1,2\",)\n",
    "])\n",
    "\n",
    "prompt=\"Cost of $355.39 + $924.87 + $721,2 + $1940.29 + $573.63 + $65.72 + $35 + $552.00 + $76.16 + $29.12\"\n",
    "agent.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIFunctionsAgent\ntools -> 0\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\u001b[38;5;241m+\u001b[39mb\n\u001b[1;32m     25\u001b[0m llm\u001b[38;5;241m=\u001b[39mChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m  \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m  \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAgentType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPENAI_FUNCTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCalculatorTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost of $355.39 + $924.87 + $721,2 + $1940.29 + $573.63 + $65.72 + $35 + $552.00 + $76.16 + $29.12\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m agent\u001b[38;5;241m.\u001b[39minvoke(prompt)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/agents/initialize.py:57\u001b[0m, in \u001b[0;36minitialize_agent\u001b[0;34m(tools, llm, agent, callback_manager, agent_path, agent_kwargs, tags, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     agent_cls \u001b[38;5;241m=\u001b[39m AGENT_TO_CLASS[agent]\n\u001b[1;32m     56\u001b[0m     agent_kwargs \u001b[38;5;241m=\u001b[39m agent_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 57\u001b[0m     agent_obj \u001b[38;5;241m=\u001b[39m \u001b[43magent_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm_and_tools\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43magent_kwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m agent_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     agent_obj \u001b[38;5;241m=\u001b[39m load_agent(\n\u001b[1;32m     62\u001b[0m         agent_path, llm\u001b[38;5;241m=\u001b[39mllm, tools\u001b[38;5;241m=\u001b[39mtools, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager\n\u001b[1;32m     63\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/agents/openai_functions_agent/base.py:233\u001b[0m, in \u001b[0;36mOpenAIFunctionsAgent.from_llm_and_tools\u001b[0;34m(cls, llm, tools, callback_manager, extra_prompt_messages, system_message, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly supported with ChatOpenAI models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    229\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_prompt(\n\u001b[1;32m    230\u001b[0m     extra_prompt_messages\u001b[38;5;241m=\u001b[39mextra_prompt_messages,\n\u001b[1;32m    231\u001b[0m     system_message\u001b[38;5;241m=\u001b[39msystem_message,\n\u001b[1;32m    232\u001b[0m )\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIFunctionsAgent\ntools -> 0\n  Can't instantiate abstract class BaseTool with abstract method _run (type=type_error)"
     ]
    }
   ],
   "source": [
    "from typing import Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from pydantic import BaseModel , PositiveInt, Field\n",
    "\n",
    "\n",
    "\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "  a:float = Field(description=\"The first number\")\n",
    "  b:float = Field(description=\"The second number\")\n",
    "\n",
    "class CalculatorTool(BaseModel):\n",
    "  name=\"CalculatorTool\"\n",
    "  description=\"\"\"\n",
    "  Use this to perform sums of two numbers.\n",
    "  The first and second arguments should be numbers.\n",
    "  Only receives two arguments.\n",
    "  \"\"\"\n",
    "  args_schema: Type[CalculatorToolArgsSchema] = CalculatorToolArgsSchema\n",
    "\n",
    "  def _run(self, a,b):\n",
    "    return a+b\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "agent = initialize_agent(\n",
    "  llm=llm, \n",
    "  verbose=True,\n",
    "  agent=AgentType.OPENAI_FUNCTIONS,\n",
    "  tools=[\n",
    "    CalculatorTool()\n",
    "])\n",
    "\n",
    "prompt=\"Cost of $355.39 + $924.87 + $721,2 + $1940.29 + $573.63 + $65.72 + $35 + $552.00 + $76.16 + $29.12\"\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_agent, AgentType\n\u001b[0;32m----> 7\u001b[0m llm\u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCalculatorToolArgsSchema\u001b[39;00m(BaseModel):\n\u001b[1;32m     11\u001b[0m     a: \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm= ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "    a: float= Field(description=\"The first number\")\n",
    "    b: float= Field(description=\"The second number\")\n",
    "\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name= \"CalculatorTool\"\n",
    "    description= \"\"\"\n",
    "    Use this to perform sums of two numbers.\n",
    "    The first and second arguments should be numbers.\n",
    "    Only receives two arguments.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CalculatorToolArgsSchema]= CalculatorToolArgsSchema\n",
    "\n",
    "    def _run(self, a, b):\n",
    "      return a+ b\n",
    "\n",
    "\n",
    "agent= initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        CalculatorTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt= \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `StockMarketSymbolSearchTool` with `{'query': 'CloudFlare stock'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m\n\u001b[1;32m     27\u001b[0m agent\u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[1;32m     28\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     29\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     ],\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m prompt\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive me information on CloudFlare\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms stock and help me analyze if it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a potential good investment. Also tell me what symbol does the stock have\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/chains/base.py:87\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     83\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     85\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     86\u001b[0m     config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/agents/agent.py:1146\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1146\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1155\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1156\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/agents/agent.py:996\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    994\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/tools/base.py:365\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    364\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    369\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/tools/base.py:339\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    336\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ToolException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_tool_error:\n",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m, in \u001b[0;36mStockMarketSymbolSearchTool._run\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, query):\n\u001b[1;32m     22\u001b[0m   ddg\u001b[38;5;241m=\u001b[39mDuckDuckGoSearchAPIWrapper()\n\u001b[0;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mddg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/utilities/duckduckgo_search.py:61\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.run\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     snippets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_snippets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(snippets)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/langchain/utilities/duckduckgo_search.py:53\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.get_snippets\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo good DuckDuckGo Search Result was found\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m snippets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         snippets\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:150\u001b[0m, in \u001b[0;36mDDGS.text\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_api(keywords, region, safesearch, timelimit)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_html(keywords, region, safesearch, timelimit)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:201\u001b[0m, in \u001b[0;36mDDGS._text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m70\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m120\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    200\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m s\n\u001b[0;32m--> 201\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://links.duckduckgo.com/d.js\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:89\u001b[0m, in \u001b[0;36mDDGS._get_url\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_get_url() \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ex)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m418\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ex):\n\u001b[0;32m---> 89\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m     90\u001b[0m     sleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt-1th/env/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:82\u001b[0m, in \u001b[0;36mDDGS._get_url\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m     79\u001b[0m     method, url, follow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_500_in_url(\u001b[38;5;28mstr\u001b[39m(resp\u001b[38;5;241m.\u001b[39murl)) \u001b[38;5;129;01mor\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m202\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39m_exceptions\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mHTTPError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "llm= ChatOpenAI(temperature=0.1)\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "  query:str=Field(description=\"The query you will search for\")\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "  name=\"StockMarketSymbolSearchTool\"\n",
    "  description=\"\"\"\n",
    "  Use this tool to find the stock market symbol for a company.\n",
    "  It takes a query as an argument.\n",
    "  Example query: Stock market Symbol for Apple Company\n",
    "  \"\"\"\n",
    "  args_schema:Type[StockMarketSymbolSearchToolArgsSchema]=StockMarketSymbolSearchToolArgsSchema\n",
    "\n",
    "  def _run(self, query):\n",
    "    ddg=DuckDuckGoSearchAPIWrapper()\n",
    "    return ddg.run(query)\n",
    "\n",
    "\n",
    "\n",
    "agent= initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        StockMarketSymbolSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt= \"Give me information on CloudFlare's stock and help me analyze if it's a potential good investment. Also tell me what symbol does the stock have\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `WikipediaSearchTool` with `{'query': 'xz backdoor'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: XZ Utils backdoor\n",
      "Summary: On 29 March 2024, software developer Andres Freund reported that he had found a maliciously introduced backdoor in the Linux utility xz within the liblzma library in versions 5.6.0 and 5.6.1 released in February 2024.While xz is commonly present in most Linux distributions, the backdoor only targeted Debian- and RPM-based systems running on the x86-64 architecture. At the time of discovery the backdoored version had not yet been widely deployed to production systems, but was present in development versions of major distributions.The backdoor gives an attacker who possesses a specific Ed448 private key remote code execution capabilities on the affected Linux system. The issue has been assigned a CVSS score of 10.0, the highest possible score.\n",
      "\n",
      "Page: XZ Utils\n",
      "Summary: XZ Utils (previously LZMA Utils) is a set of free software command-line lossless data compressors, including the programs lzma and xz, for Unix-like operating systems and, from version 5.0 onwards, Microsoft Windows. For compression/decompression the Lempel–Ziv–Markov chain algorithm (LZMA) is used. XZ Utils started as a Unix port of Igor Pavlov's LZMA-SDK that has been adapted to fit seamlessly into Unix environments and their usual structure and behavior.\n",
      "\n",
      "Page: Backdoor (computing)\n",
      "Summary: A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\"—a tiny computer-within-a-computer such as that found in Intel's AMT technology). Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\n",
      "A backdoor may take the form of a hidden part of a program, a separate program (e.g. Back Orifice may subvert the system through a rootkit), code in the firmware of the hardware, or parts of an operating system such as Windows. Trojan horses can be used to create vulnerabilities in a device. A Trojan horse may appear to be an entirely legitimate program, but when executed, it triggers an activity that may install a backdoor. Although some are secretly installed, other backdoors are deliberate and widely known. These kinds of backdoors have \"legitimate\" uses such as providing the manufacturer with a way to restore user passwords.\n",
      "Many systems that store information within the cloud fail to create accurate security measures. If many systems are connected within the cloud, hackers can gain access to all other platforms through the most vulnerable system. Default passwords (or other default credentials) can function as backdoors if they are not changed by the user. Some debugging features can also act as backdoors if they are not removed in the release version. In 1993, the United States government attempted to deploy an encryption system, the Clipper chip, with an explicit backdoor for law enforcement and national security access. The chip was unsuccessful.Recent proposals to counter backdoors include creating a database of backdoors' triggers and then using neural networks to detect them.\u001b[0m\u001b[32;1m\u001b[1;3mI found information about the xz backdoor. On 29 March 2024, a backdoor was discovered in the Linux utility xz within the liblzma library in versions 5.6.0 and 5.6.1. The backdoor targeted Debian- and RPM-based systems running on the x86-64 architecture. It allowed an attacker with a specific Ed448 private key to execute remote code on the affected Linux system. The issue was assigned a CVSS score of 10.0, the highest possible score.\n",
      "\n",
      "XZ Utils is a set of free software command-line lossless data compressors that includes programs like lzma and xz. It uses the Lempel–Ziv–Markov chain algorithm (LZMA) for compression/decompression.\n",
      "\n",
      "A backdoor is a covert method of bypassing normal authentication or encryption in a computer system. It can be used for securing remote access or obtaining access to plaintext in cryptosystems. Backdoors can take various forms, such as hidden parts of a program, separate programs, firmware code, or parts of an operating system.\n",
      "\n",
      "If you need more information or have any specific questions, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Research about the xz backdoor',\n",
       " 'output': 'I found information about the xz backdoor. On 29 March 2024, a backdoor was discovered in the Linux utility xz within the liblzma library in versions 5.6.0 and 5.6.1. The backdoor targeted Debian- and RPM-based systems running on the x86-64 architecture. It allowed an attacker with a specific Ed448 private key to execute remote code on the affected Linux system. The issue was assigned a CVSS score of 10.0, the highest possible score.\\n\\nXZ Utils is a set of free software command-line lossless data compressors that includes programs like lzma and xz. It uses the Lempel–Ziv–Markov chain algorithm (LZMA) for compression/decompression.\\n\\nA backdoor is a covert method of bypassing normal authentication or encryption in a computer system. It can be used for securing remote access or obtaining access to plaintext in cryptosystems. Backdoors can take various forms, such as hidden parts of a program, separate programs, firmware code, or parts of an operating system.\\n\\nIf you need more information or have any specific questions, feel free to ask!'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class WikipediaSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=\"The query you will search for.Example query: Research about the xz backdoor\"\n",
    "    )\n",
    "\n",
    "\n",
    "class WikipediaSearchTool(BaseTool):\n",
    "    name = \"WikipediaSearchTool\"\n",
    "    description = \"\"\"\n",
    "    Use this tool to find avout Research about the xz backdoor.\n",
    "    It takes a query as an argument.\n",
    "    \n",
    "    \"\"\"\n",
    "    args_schema: Type[\n",
    "        WikipediaSearchToolArgsSchema\n",
    "    ] = WikipediaSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query):\n",
    "        wiki = WikipediaAPIWrapper()\n",
    "        return wiki.run(query)\n",
    "    # def _run(self, query):\n",
    "    #     ddg = DuckDuckGoSearchResults()\n",
    "    #     return ddg.run(query)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        WikipediaSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Research about the xz backdoor\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
